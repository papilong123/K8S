############################# Broker 高 ##########################################################

# Zookeeper 主机地址，如果有多个使用逗号分割
zookeeper.connect=localhost:2181
# 监听器列表 - 使用逗号分隔URI列表和监听器名称
listeners= "PLAINTEXT://0.0.0.0:9092"
# 监听器发布到ZooKeeper供客户端使用
advertised.listeners= "PLAINTEXT://localhost:9092"
# 是否允许自动创建topic ，若是false，就需要通过命令创建topic,默认为true
auto.create.topics.enable=true
# 是否自动平衡broker之间的分配策略,生产环境下建议设置为false
auto.leader.rebalance.enable = false
# 默认10 用于处理各种后台任务的线程数量
background.threads = 2
# 唯一标识在集群中的ID，要求是正数。
broker.id=0
# 为特点的topic指定一个最终压缩类型。此配置接受的标准压缩编码方式有（"gzip", "snappy", "lz4"）。此外还有“uncompressed”相当于不压缩；”producer“意味着压缩类型由producer决定
compression.type=producer
# 是否允许删除topic。如果关闭此配置，通过管理工具删除topic将不再生效
delete.topic.enable=true

# 默认300 检查leader是否不平衡的时间间隔
leader.imbalance.check.interval.seconds = 300
# 默认10 每个broker允许的不平衡的leader的百分比，如果高于这个比值将触发leader进行平衡。这个值用百分比来指定
leader.imbalance.per.broker.percentage = 10

# 日志存放目录，多个目录使用逗号分割
log.dirs=/kafka/kafka-logs
# 日志存放目录（对log.dirs属性的补充）
log.dir=

# 默认9223372036854775807 在将消息刷新到磁盘之前，在日志分区上累积的消息数量
log.flush.interval.messages=9223372036854775807
# 默认null 在刷新到磁盘之前，任何topic中的消息保留在内存中的最长时间（以毫秒为单位）。如果未设置，则使用log.flush.scheduler.interval.ms中的值。
log.flush.interval.ms=
# 默认值:60000 记录上次把log刷到磁盘的时间点的频率,用来日后的recovery。通常不需要改变
log.flush.start.offset.checkpoint.interval.ms=60000
# 默认9223372036854775807ms 当达到下面的时间(ms)时，执行一次强制的flush操作。interval.ms和interval.messages无论哪个达到，都会flush。
log.flush.scheduler.interval.ms=9223372036854775807

# 默认-1 表示没有限制，日志删除的大小阈值
log.retention.bytes=-1
# 默认为7天（168小时） 日志保存时间 (hours|minutes)，超过这个时间会根据policy处理数据。bytes和minutes无论哪个先达到都会触发。
log.retention.hours=168
# 默认null 日志删除的时间阈值（分钟为单位），如果未设置，将使用log.retention.hours的值
log.retention.minutes=null

# 新日志段轮转时间间隔（小时为单位），次要配置为log.roll.ms,当达到下面时间，会强制新建一个segment
log.roll.hours=168
# 默认0 从logrolltimemillis（以小时计）中减去的最大抖动，int类型，次要配置log.roll.jitter.ms
log.roll.jitter.hours=0
# 默认null 从logrolltimemillis（以毫秒计）中减去的最大抖动，float类型，如果未设置，则使用log.roll.jitter.hours的配置
log.roll.jitter.ms=null
# 默认null 新日志段轮转时间间隔（毫秒为单位），如果未设置，则使用log.roll.hours配置
log.roll.ms=null

# 默认1073741824Byte 单个日志段文件最大大小，超出该大小则追加到一个新的日志segment文件中（-1表示没有限制）
log.segment.bytes=1073741824
# 默认60000 long类型 从文件系统中删除一个日志段文件前的保留时间
log.segment.delete.delay.ms=60000

# 默认1000012 int类型 消息体的最大大小，单位是字节，默认大概为1MB
# kafka允许的最大的一个批次的消息大小。 如果这个数字增加，且有0.10.2版本以下的consumer
# 那么consumer的提取大小也必须增加，以便他们可以取得这么大的记录批次
# 在最新的消息格式版本中，记录总是被组合到一个批次以提高效率
# 在以前的消息格式版本中，未压缩的记录不会分组到批次中，并且此限制仅适用于该情况下的单个记录
# 可以使用topic设置`max.message.bytes`来设置每个topic。 max.message.bytes
message.max.bytes=1000012

# 默认1 当producer将ack设置为“all”（或“-1”）时，min.insync.replicas指定了被认为写入成功的最小副本数
# 如果这个最小值不能满足，那么producer将会引发一个异常（NotEnoughReplicas或NotEnoughReplicasAfterAppend）
# min.insync.replicas和acks一起使用时允许您强制更大的持久性保证
# 一个经典的情况是创建一个复本数为3的topic，将min.insync.replicas设置为2，并且producer使用“all”选项
# 这将确保如果大多数副本没有写入producer则抛出异常
min.insync.replicas=1

# 默认8 处理磁盘I/O的线程数，服务器用于处理请求的线程数，可能包括磁盘I/O
num.io.threads=8
# 默认3 处理网络请求的最大线程数，服务器用于从接收网络请求并发送网络响应的线程数
num.network.threads=2
# 默认1 从源broker复制消息的拉取器的线程数。增加这个值可以增加follow broker的I/O并行度,说白了就是默认创建的副本数量
num.replica.fetchers=1

# 客户端保留offset信息的最大空间大小,与offset提交相关联的元数据条目的最大大小
offset.metadata.max.bytes = 1024
# 在offset提交可以接受之前，需要设置acks的数目，一般不需要更改，默认值为-1
offsets.commit.required.acks=-1
# offset提交将延迟到topic所有副本收到提交或超时。这与producer请求超时类似
offsets.commit.timeout.ms=5000
# 每次从offset段文件往缓存加载时，批量读取的数据大小
offsets.load.buffer.size=5242880
# 检查失效offset的频率
offsets.retention.check.interval.ms=600000
# 超过这个保留期限未提交的offset将被丢弃
offsets.retention.minutes=1440
# 用于offsets topic的压缩编解码器 - 压缩可用于实现“原子”提交
offsets.topic.compression.codec=0
# offsets topic的分区数量（部署后不应更改）
offsets.topic.num.partitions=50
# offset topic的副本数（设置的越大，可用性越高）。内部topic创建将失败，直到集群大小满足此副本数要求
offsets.topic.replication.factor=3
# 为了便于更快的日志压缩和缓存加载，offset topic段字节应该保持相对较小
offsets.topic.segment.bytes=104857600

# 默认500 网络线程阻塞前队列允许的最大请求数，等待IO线程处理的请求队列最大数,为了便于更快的日志压缩和缓存加载，offset topic段字节应该保持相对较小
queued.max.requests = 500

# 默认1 每一个fetch操作的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会等待直到数据达到这个大小
# 复制数据过程中，replica收到的每个fetch响应，期望的最小的字节数，如果没有收到足够的字节数，就会等待更多的数据，直到达到replicaMaxWaitTimeMs（复制数据超时时间）
replica.fetch.min.bytes=1
# 默认500 副本follow同leader之间通信的最大等待时间，失败了会重试。 此值始终应始终小于replica.lag.time.max.ms，以防止针对低吞吐量topic频繁收缩ISR
# replicas同leader之间通信的最大等待时间，失败了会重试
replica.fetch.wait.max.ms=500
# high watermark被保存到磁盘的频率，用来标记日后恢复点
# 每个replica将最高水位进行flush的时间间隔
replica.high.watermark.checkpoint.interval.ms=5000
# 默认10000ms 如果一个follower在这个时间内没有发送fetch请求或消费leader日志到结束的offset，leader将从ISR中移除这个follower，并认为这个follower已经挂了
# replicas响应leader的最长等待时间，若是超过这个时间，就将replicas排除在管理之外
replica.lag.time.max.ms=10000
# 默认64 * 1024 socket接收网络请求的缓存大小
# leader复制的socket缓存大小
replica.socket.receive.buffer.bytes=65536
# 副本复制数据过程中，发送网络请求的socket超时时间。这个值应该大于replica.fetch.wait.max.ms的值
# leader与relicas的socket超时时间
replica.socket.timeout.ms=30000

# 默认30s 该配置控制客户端等待请求响应的最长时间。如果在超时之前未收到响应，则客户端将在必要时重新发送请求，如果重试仍然失败，则请求失败。
# 消息发送的最长等待时间
request.timeout.ms=30000

# 默认102400 socket的接收缓冲区 (SO_RCVBUF)
# 服务端用来处理socket连接的SO_RCVBUFF缓冲大小。如果值为-1，则使用系统默认值。
socket.receive.buffer.bytes=102400
# 默认104857600 socket请求的最大字节数。为了防止内存溢出，message.max.bytes必然要小于
# socket请求的最大大小，这是为了防止server跑光内存，不能大于Java堆的大小
socket.request.max.bytes = 104857600
# 默认102400 服务端用来处理socket连接的SO_SNDBUF缓冲大小。如果值为-1，则使用系统默认值
# socket的发送缓冲区（SO_SNDBUF）
socket.send.buffer.bytes=102400

# 默认900000 事务允许的最大超时时间。如果客户请求的事务超时，那么broker将在InitProducerIdRequest中返回一错误
# 这样可以防止客户超时时间过长，从而阻碍consumers读取事务中包含的topic
transaction.max.timeout.ms=900000
# 默认5242880 将producer ID和事务加载到高速缓存中时，从事务日志段（the transaction log segments）中批量读取的大小
transaction.state.log.load.buffer.size=5242880
# 默认2 覆盖事务topic的min.insync.replicas配置
transaction.state.log.min.isr=2
# 事务topic的分区数（部署后不应该修改）
transaction.state.log.num.partitions=50
# 默认3 事务topic的副本数（设置的越大，可用性越高）。内部topic在集群数满足副本数之前，将会一直创建失败
transaction.state.log.replication.factor=3
# 默认104857600 事务topic段应保持相对较小，以便于更快的日志压缩和缓存负载。
transaction.state.log.segment.bytes=104857600
# 默认604800000 事务协调器在未收到任何事务状态更新之前，主动设置producer的事务标识为过期之前将等待的最长时间（以毫秒为单位）
transactional.id.expiration.ms=604800000

# 默认false 指定副本是否能够不再ISR中被选举为leader，即使这样可能会丢数据
unclean.leader.election.enable=false

# 默认null 连接zk的超时时间
# 与ZK server建立连接的超时时间,没有配置就使用zookeeper.session.timeout.ms
zookeeper.connection.timeout.ms=10000
# 默认6000 ZooKeeper的session的超时时间
# zookeeper的心跳超时时间，超过这个时间就认为是无效的消费者
zookeeper.session.timeout.ms=6000
# ZooKeeper客户端连接是否设置ACL安全连接
zookeeper.set.acl=false

########################################### Broker 中 #############################################

# 默认true 是否允许服务器自动生成broker.id。如果允许则产生的值会交由reserved.broker.max.id审核
broker.id.generation.enable=true
# 默认null broker的机架位置。 这将在机架感知副本分配中用于容错。例如：RACK1，us-east-1
broker.rack=null
# 默认60s 连接空闲超时：服务器socket处理线程空闲超时关闭时间
connections.max.idle.ms=600000
# 默认true 是否允许服务器关闭broker服务，若是设置为true,会关闭所有在这个broker上的leader，并转移到其他broker

controlled.shutdown.enable=true
# 默认3 当发生失败故障时，由于各种原因导致关闭服务的次数，控制器关闭的尝试次数
controlled.shutdown.max.retries=3
# 默认5s 在每次重试关闭之前，系统需要时间从上次故障状态（控制器故障切换，副本延迟等）中恢复。 这个配置决定了重试之前等待的时间
# 每次关闭尝试的时间间隔
controlled.shutdown.retry.backoff.ms=5000
# 默认30s 控制器到broker通道的socket超时时间，partition management controller 与replicas之间通讯的超时时间
controller.socket.timeout.ms=30000

# 默认1 自动创建topic时的默认副本个数，一个topic ，默认分区的replication个数 ，不能大于集群中broker的个数
default.replication.factor=1
# 默认1 删除purgatory中请求的清理间隔时间
#（purgatory：broker对于无法立即处理的请求，将会放在purgatory中，当请求完成后，并不会立即清除，
# 还会继续在purgatory中占用资源，直到下一次delete.records.purgatory.purge.interval.requests）
delete.records.purgatory.purge.interval.requests=1

# 默认1000 提取purgatory中请求的间隔时间
fetch.purgatory.purge.interval.requests=1000






# 默认1 每个topic的分区个数，更多的partition会产生更多的segment file
num.partitions=1

########################################### Broker 低 #############################################







# 日志清理策略（delete|compact）
log.cleanup.policy = compact


# 日志数据存储的最大字节数。超过这个时间会根据policy处理数据。
#log.retention.bytes=1073741824

# 日志片段文件的检查周期，查看它们是否达到了删除策略的设置（log.retention.hours或log.retention.bytes）
log.retention.check.interval.ms=60000

# 是否开启压缩
log.cleaner.enable=false
# 对于压缩的日志保留的最长时间
log.cleaner.delete.retention.ms = 1 day

# 对于segment日志的索引文件大小限制
log.index.size.max.bytes = 10 * 1024 * 1024
#y索引计算的一个缓冲区，一般不需要设置。
log.index.interval.bytes = 4096



# controller-to-broker-channels消息队列的尺寸大小
controller.message.queue.size=10



# 如果relicas落后太多,将会认为此partition relicas已经失效。而一般情况下,因为网络延迟等原因,总会导致replicas中消息同步滞后。如果消息严重滞后,leader将认为此relicas网络延迟较大或者消息吞吐能力有限。在broker数量较少,或者网络不足的环境中,建议提高此值.
replica.lag.max.messages = 4000

# replicas每次获取数据的最大字节数
replica.fetch.max.bytes = 1024 * 1024


############################# Topic ##############################################################

# 与Topic相关的配置既包含服务器默认值，也包含可选的每个Topic覆盖值
# 如果没有给出每个Topic的配置，那么服务器默认值就会被使用。通过提供一个或多个 --config 选项，可以在创建Topic时设置覆盖值



# 默认2s ZooKeeper集群follower同步可落后leader多久
zookeeper.sync.time.ms = 2000
############################# Producer 高 ##########################################################
# 核心的配置包括：
# metadata.broker.list
# request.required.acks
# producer.type
# serializer.class

# 消费者获取消息元信息(topics, partitions and replicas)的地址,配置格式是：host1:port1,host2:port2，也可以在外面设置一个vip
metadata.broker.list=

#消息的确认模式
# 0：不保证消息的到达确认，只管发送，低延迟但是会出现消息的丢失，在某个server失败的情况下，有点像TCP
# 1：发送消息，并会等待leader 收到确认后，一定的可靠性
# -1：发送消息，等待leader收到确认，并进行复制操作后，才返回，最高的可靠性
request.required.acks = 1



# socket的缓存大小
send.buffer.bytes=100*1024
# key的序列化方式，若是没有设置，同serializer.class
key.serializer.class=org.apache.kafka.common.serialization.StringSerializer
# value的序列化方式，若是没有设置，同serializer.class
value-serializer = org.apache.kafka.common.serialization.StringSerializer
# 分区的策略，默认是取模
partitioner.class=kafka.producer.DefaultPartitioner
# 消息的压缩模式，默认是none，可以有gzip和snappy
compression.codec = none
# 可以针对默写特定的topic进行压缩
compressed.topics=null
# 消息发送失败后的重试次数
message.send.max.retries = 3
# 每次失败后的间隔时间
retry.backoff.ms = 100
# 生产者定时更新topic元信息的时间间隔 ，若是设置为0，那么会在每个消息发送后都去更新数据
topic.metadata.refresh.interval.ms = 600 * 1000

# 异步模式下缓冲数据的最大时间。例如设置为100则会集合100ms内的消息后发送，这样会提高吞吐量，但是会增加消息发送的延时
queue.buffering.max.ms = 5000
# 异步模式下缓冲的最大消息数，同上
queue.buffering.max.messages = 10000
# 异步模式下，消息进入队列的等待时间。若是设置为0，则消息不等待，如果进入不了队列，则直接被抛弃
queue.enqueue.timeout.ms = -1
# 异步模式下，每次发送的消息数，当queue.buffering.max.messages或queue.buffering.max.ms满足条件之一时producer会触发发送。
batch.num.messages=200

############################# Consumer 高 ##########################################################

############################# Consumer 中 ##########################################################
# 默认"" Consumer端核心的配置是group.id、zookeeper.connect
# 决定该Consumer归属的唯一组ID，By setting the same group id multiple processes indicate that they are all part of the same consumer group.
group.id=defaultGroupId
# 消费者的ID，若是没有设置的话，会自增
consumer.id=defaultConsumerId
# 一个用于跟踪调查的ID ，最好同group.id相同
client.id = <group_id>

# 默认class org.apache.kafka.clients.consumer.RangeAssignor list类型
partition.assignment.strategy=class org.apache.kafka.clients.consumer.RangeAssignor

# 当zookeeper中没有初始的offset时，或者超出offset上限时的处理方式 。
# smallest ：重置为最小值
# largest:重置为最大值
# anything else：抛出异常给consumer
auto.offset.reset = largest

# socket的超时时间，实际的超时时间为max.fetch.wait + socket.timeout.ms.
socket.timeout.ms= 30 * 1000
#从每个分区fetch的消息大小限制
fetch.message.max.bytes = 1024 * 1024

# true时，Consumer会在消费消息后将offset同步到zookeeper，这样当Consumer失败后，新的consumer就能从zookeeper获取最新的offset
auto.commit.enable = true
# 自动提交的时间间隔
auto.commit.interval.ms = 60 * 1000

# 用于消费的最大数量的消息块缓冲大小，每个块可以等同于fetch.message.max.bytes中数值
queued.max.message.chunks = 10

# 当有新的consumer加入到group时,将尝试reblance,将partitions的消费端迁移到新的consumer中, 该设置是尝试的次数
rebalance.max.retries = 4
# 每次reblance的时间间隔
rebalance.backoff.ms = 2000
# 每次重新选举leader的时间
refresh.leader.backoff.ms=

# server发送到消费端的最小数据，若是不满足这个数值则会等待直到满足指定大小。默认为1表示立即接收。
fetch.min.bytes = 1
# 若是不满足fetch.min.bytes时，等待消费端请求的最长等待时间
fetch.wait.max.ms = 100
# 如果指定时间内没有新消息可用于消费，就抛出异常，默认-1表示不受限
consumer.timeout.ms = -1



############################# Connect ############################################################
############################# Streams ############################################################
############################# AdminClient ########################################################



